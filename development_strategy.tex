\subsection{Development strategy}

\textbf{Virtual environment--driven design and testing in Unity.} A virtual home-office scene will be created in Unity with realistic layouts that include a desk, shelf, whiteboard, and documents. This environment supports repeatable testing because the position and identity of each object are known. Ground-truth labels, such as object classes and bounding boxes, can be generated directly from the scene, making it possible to evaluate detection accuracy without collecting real personal documents. The virtual environment will be used to sweep stress conditions such as lighting, clutter, distance, motion, and camera angle. Rendered frames from a Unity camera will be fed into the same detection pipeline that will later run on Quest~3, keeping evaluation consistent across simulated and real settings.

\textbf{Sensitive-artifact detection using YOLO with ONNX Runtime in Unity.} The detection module will use a YOLO-style object detector to identify sensitive physical artifacts such as documents, envelopes, notebooks, and whiteboards. The model will be developed in Python using the Ultralytics YOLO training workflow and then exported to ONNX format for deployment. On Meta Quest~3, inference will run inside the Unity application using ONNX Runtime for Unity. Frames will be downsampled to reduce computational cost. The detector will output bounding boxes, class labels, and confidence scores, from which a risk score will be computed.

\textbf{Real-time mitigation implemented with overlays.} Mitigation will be applied directly in the rendering pipeline of the Unity-based Quest app. When the risk score exceeds a threshold, the system will apply region-based blur over detected bounding boxes. Users will be prompted to reposition objects to reduce risk scores. Mitigation will be more aggressive when sharing is active and less intrusive during normal interaction.

\textbf{Policy, user controls, and transparent feedback as usable security.} A small policy layer will control when detection runs and how mitigation is applied. The primary interface will be a \emph{work mode} toggle that enables stricter protection during productivity and communication scenarios. Controls will be designed for fast use, including quick override and mitigation preference settings, to reduce interruption and prevent prompt fatigue.

\textbf{Camera-access audit logging stored locally as structured events.} The system will record an audit log as a local JSONL file stored on-device, where each line is a structured event. Events will include timestamps, session identifiers, active app context, and any media-save actions. Each event will also include an event summary and a risk level. The log will avoid storing raw frames. A simple review interface will present recent events as a timeline so users can understand what happened and investigators can reconstruct exposure incidents.

\textbf{Evaluation metrics.} The system will be evaluated using three classes of metrics. Privacy and security outcomes will measure exposure reduction, such as how often sensitive artifacts are visible without mitigation and how long they remain readable. Detection quality will be measured with precision and recall for target artifact classes in the virtual environment and in a limited set of controlled Quest~3 tests. System performance will measure end-to-end latency, CPU and memory usage, and battery impact. The audit log will be evaluated for forensic usefulness.
